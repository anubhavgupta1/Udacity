{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled25.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfo7SojIW6GtGc6Q09jy7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Udacity/blob/main/Frameworks/Tensorflow/RNNs/Char%20RNN/shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3IafOoJONYV",
        "outputId": "c893b81a-1be2-4707-a7e9-5e7af5097fa7"
      },
      "source": [
        "!ls\r\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\r\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\r\n",
        "!apt-get update -qq 2>&1 > /dev/null\r\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\r\n",
        "from google.colab import auth\r\n",
        "auth.authenticate_user()\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "creds = GoogleCredentials.get_application_default()\r\n",
        "import getpass\r\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\r\n",
        "vcode = getpass.getpass()\r\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\r\n",
        "\r\n",
        "\r\n",
        "!mkdir -p drive\r\n",
        "!google-drive-ocamlfuse drive\r\n",
        "\r\n",
        "import os\r\n",
        "os.chdir(\"drive/Udacity/RNN/Tensorflow Shakespear/\")\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 146364 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.24-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ogft4_z9Ujz"
      },
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JaMz8Ua9TUt"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lSzeTk49cAO"
      },
      "source": [
        "## Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R5fADzw9dIk",
        "outputId": "f11458b5-30f1-4385-dd9a-9f5eea2c8be3"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGN31akH9pRR"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUo1aUNS9mvD",
        "outputId": "0e2232bd-f86d-4207-90f5-880a8b48d5b6"
      },
      "source": [
        "# Read, then decode for py2 compat.\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print('Length of text: {} characters'.format(len(text)))\r\n",
        "# Take a look at the first 250 characters in text\r\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5QsJBJe96_E",
        "outputId": "0c340300-c8da-4214-a648-27d016bd1ae2"
      },
      "source": [
        "# The unique characters in the file\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDw2km6-Dec"
      },
      "source": [
        "# Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INgHDR4t-F6b"
      },
      "source": [
        "## Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh4fjJhj-FUI",
        "outputId": "7b19f0e1-52bc-4607-8c72-89b70fe5eab1"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\r\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\r\n",
        "print(chars)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgXh4jM--knQ"
      },
      "source": [
        "#Now create the preprocessing.StringLookup layer\r\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SuX-Pun_TwX",
        "outputId": "e4772a17-70d9-4e98-8aa2-f5f729780368"
      },
      "source": [
        "#It converts form tokens to character IDs, padding with 0:\r\n",
        "ids = ids_from_chars(chars)\r\n",
        "print(ids)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9cMPUqM_y9k",
        "outputId": "4558aad9-9941-482b-d52d-dc6b84efc5b2"
      },
      "source": [
        "print(ids_from_chars.get_vocabulary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', '\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2BDvnqF_sSv"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup( vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fd1z7_VA1bh",
        "outputId": "f430914f-b2c7-4d1f-b966-f0f000a2bd63"
      },
      "source": [
        "chars = chars_from_ids(ids)\r\n",
        "print(chars)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V4AcmGuAQ3p",
        "outputId": "72193670-5da5-4215-d881-12caaec8558e"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUBFFQlRAncv"
      },
      "source": [
        "def text_from_ids(ids):\r\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-TFGS7OA_J-"
      },
      "source": [
        "## Create training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq0_tX9SBAB6",
        "outputId": "b16451a6-060e-41e6-edc6-67af7bbbd3b8"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\r\n",
        "print(all_ids)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([20 49 58 ... 47 10  2], shape=(1115394,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvjzRLI0BQ_a",
        "outputId": "e55775ea-9033-4cf5-d0b0-d8ca6f9997d8"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\r\n",
        "print(ids_dataset)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXz4EMASBfPE",
        "outputId": "6d5b1ca8-6d3d-44dc-d721-f1b00e55bd25"
      },
      "source": [
        "for ids in ids_dataset.take(13):\r\n",
        "    chars_from_ids_ = chars_from_ids(ids)\r\n",
        "    print(chars_from_ids_.numpy().decode('utf-8'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unrsSkRgB7de",
        "outputId": "f33a0eda-1386-4ded-fbe6-4384e39e08a2"
      },
      "source": [
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "print(examples_per_epoch)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w3jYKV6CW8m",
        "outputId": "760589e4-5868-45d0-f8a9-efa446f919c3"
      },
      "source": [
        "#The batch method lets you easily convert these individual characters to sequences of the desired size\r\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "for seq in sequences.take(1):\r\n",
        "  print(\"Sequence is : \")\r\n",
        "  print(seq)\r\n",
        "  print(\"\\nCharacter in sequence are : \")\r\n",
        "  print(chars_from_ids(seq))\r\n",
        "  print(\"\\nText in sequence is : \")\r\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence is : \n",
            "tf.Tensor(\n",
            "[20 49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45\n",
            "  3 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45\n",
            " 41 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51\n",
            "  8  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12\n",
            "  2 39 55 61  3], shape=(101,), dtype=int64)\n",
            "\n",
            "Character in sequence are : \n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "\n",
            "Text in sequence is : \n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2izGfgoDDypq"
      },
      "source": [
        "def split_input_target(sequence):\r\n",
        "    input_text = sequence[:-1]\r\n",
        "    target_text = sequence[1:]\r\n",
        "    return input_text, target_text"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuLcWbESD25W",
        "outputId": "14b5d64e-ed36-4b1c-9550-5c77526536ff"
      },
      "source": [
        "for seq in sequences.take(1):\r\n",
        "  print(\"\\nText in sequence is : \")\r\n",
        "  _text_ = text_from_ids(seq).numpy()\r\n",
        "  print(_text_)\r\n",
        "  _input_text_, _target_text_ = split_input_target(_text_)\r\n",
        "  print(\"\\nInput Text is : \")\r\n",
        "  print(_input_text_)\r\n",
        "  print(\"\\nTarget Text is : \")\r\n",
        "  print(_target_text_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Text in sequence is : \n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "\n",
            "Input Text is : \n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "\n",
            "Target Text is : \n",
            "b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZf0eiu-FgUR"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOE5U-fyFK09",
        "outputId": "1af34c29-1de9-46c2-8b7a-f017a7a3d01f"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\r\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\r\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqccdgEtFtFQ"
      },
      "source": [
        "## Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-gn_spnFuCo",
        "outputId": "315e3b87-c88e-4ef5-e62c-35f50bb85b98"
      },
      "source": [
        "# Batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# Buffer size to shuffle the dataset\r\n",
        "# (TF data is designed to work with possibly infinite sequences,\r\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "# it maintains a buffer in which it shuffles elements).\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = (\r\n",
        "    dataset\r\n",
        "    .shuffle(BUFFER_SIZE)\r\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
        "\r\n",
        "print(dataset)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZSEqTEWG3C4"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qev-zf-DGgV-"
      },
      "source": [
        "# Length of the vocabulary in chars\r\n",
        "vocab_size = len(vocab)\r\n",
        "\r\n",
        "# The embedding dimension\r\n",
        "embedding_dim = 256\r\n",
        "\r\n",
        "# Number of RNN units\r\n",
        "rnn_units = 1024"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MfTEXYgGjky"
      },
      "source": [
        "class MyModel(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "    super().__init__(self)\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                   return_sequences=True, \r\n",
        "                                   return_state=True)\r\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "    x = inputs\r\n",
        "    x = self.embedding(x, training=training)\r\n",
        "    if states is None:\r\n",
        "      states = self.gru.get_initial_state(x)\r\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\r\n",
        "    x = self.dense(x, training=training)\r\n",
        "\r\n",
        "    if return_state:\r\n",
        "      return x, states\r\n",
        "    else: \r\n",
        "      return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vieo0_f_Gtir"
      },
      "source": [
        "model = MyModel(\r\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=embedding_dim,\r\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9vfzFW4HEcG"
      },
      "source": [
        "## Try the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTGNp7LmHAAf",
        "outputId": "30a4706a-8580-437a-9b6e-0288bc636924"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    print(input_example_batch.shape, target_example_batch.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100) (64, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tte8pkhHVVQ",
        "outputId": "722cdbe2-6bea-484b-ff40-a93ae6f75f7f"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_example_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBWxaOD0HsvJ",
        "outputId": "eb0490c5-abb1-4bf5-d7d2-88cf489d1654"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEhK8ieMH-SV",
        "outputId": "b5892ae8-8895-4712-e80b-dcd8cb47e151"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n",
        "print(sampled_indices)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[53 18 15  9 17 53 22 60  9 53 66  7 59 50  3 66 48 39 50 37 29 33 52 33\n",
            " 10 51 49  9 39 20 52 47 19 47 54 39 39 11 31 50  6 49 55 55 22  7 27  7\n",
            " 52 44 31 48 47 55  2 26 18 36 29 57 15 46 39 29 49 50 64  6 56  3 39 20\n",
            " 55 18 26 47 34 54 20  1 35  6 63 27 25 32 49 64 64  8 59 63 43 24 62  9\n",
            " 19 29 53 29]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUq462zOIXn0",
        "outputId": "94b2650c-05e3-4462-c895-ba22daf61145"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'ethought,--I\\ncannot tell how to term it.\\n\\nFirst Servingman:\\nHe had so; looking as it were--would I w'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"mDA-CmHt-mz'sj zhYjWOSlS.ki-YFlgEgnYY3Qj&iooH'M'ldQhgo\\nLDVOqAfYOijx&p YFoDLgTnF[UNK]U&wMKRixx,swcJv-EOmO\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB6ssMfCJCKt"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9INdJPwJDrA"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szBf-KytJJxF",
        "outputId": "047a6b7b-bf83-4f9b-c94f-0fddd2d80c6d"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "mean_loss = example_batch_loss.numpy().mean()\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.2042427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-wua_E8JWK0",
        "outputId": "eb0a87b5-d68b-41b5-b2a0-aed1990fde83"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.969864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70zqq7QVJbja"
      },
      "source": [
        "#Configure the training procedure using the tf.keras.Model.compile method. \r\n",
        "#Use tf.keras.optimizers.\r\n",
        "#Adam with default arguments and the loss function.\r\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoa2s4fPJp0-"
      },
      "source": [
        "## Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgPbBEl4JrSJ"
      },
      "source": [
        "#Use a tf.keras.callbacks.ModelCheckpoint \r\n",
        "#to ensure that checkpoints are saved during training:\r\n",
        "\r\n",
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vz0hqyJ8n-"
      },
      "source": [
        "## Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxRpThF2J4jk",
        "outputId": "af087307-1b73-430b-9d33-0223f3c04bcd"
      },
      "source": [
        "EPOCHS = 20\r\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 9s 41ms/step - loss: 3.2877\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 2.0798\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 1.7621\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 1.5769\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.4649\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.3861\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 1.3305\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.2824\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.2406\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.1954\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.1560\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.1113\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.0661\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.0193\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 0.9690\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 0.9151\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 0.8588\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 0.8052\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 0.7517\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 8s 40ms/step - loss: 0.7039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dpObkQiKYf7"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djv0oIVYKQuf"
      },
      "source": [
        "class OneStep(tf.keras.Model):\r\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\r\n",
        "    super().__init__()\r\n",
        "    self.temperature=temperature\r\n",
        "    self.model = model\r\n",
        "    self.chars_from_ids = chars_from_ids\r\n",
        "    self.ids_from_chars = ids_from_chars\r\n",
        "\r\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\r\n",
        "    sparse_mask = tf.SparseTensor(\r\n",
        "        # Put a -inf at each bad index.\r\n",
        "        values=[-float('inf')]*len(skip_ids),\r\n",
        "        indices = skip_ids,\r\n",
        "        # Match the shape to the vocabulary\r\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \r\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def generate_one_step(self, inputs, states=None):\r\n",
        "    # Convert strings to token IDs.\r\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
        "\r\n",
        "    # Run the model.\r\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \r\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \r\n",
        "                                          return_state=True)\r\n",
        "    # Only use the last prediction.\r\n",
        "    predicted_logits = predicted_logits[:, -1, :]\r\n",
        "    predicted_logits = predicted_logits/self.temperature\r\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\r\n",
        "\r\n",
        "    # Sample the output logits to generate token IDs.\r\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\r\n",
        "\r\n",
        "    # Convert from token ids to characters\r\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
        "\r\n",
        "    # Return the characters and model state.\r\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ7AWyzFKbRF"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tHIPKpfKmoX",
        "outputId": "3ef0ffc3-829b-44ab-83c8-496b81bde61a"
      },
      "source": [
        "start = time.time()\r\n",
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1000):\r\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\r\n",
        "\r\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "She is my business; but I'll go raze very little elpe,\n",
            "Upon his purple on thy body and her grave,\n",
            "To brief and told me hath true here doth here alone\n",
            "To let him and with a heart. Who's there a\n",
            "curse is ready; mother, are all fall together:\n",
            "That they this country's force smell of some\n",
            "person, whose haughty man into his.\n",
            "\n",
            "FLORIZEL:\n",
            "Dear, and beggary fair one\n",
            "gentleman that which you say so like a treecher.\n",
            "Coroerinant Clarence, both brought your valour; I will.\n",
            "Where dare discomes my prayers with the misther lites\n",
            "Twice drawn overba-store?\n",
            "\n",
            "ELBOW:\n",
            "He may, let's about it. And tell us thither.\n",
            "\n",
            "KING RICHARD III:\n",
            "But lords, we have ended me to closent to follow me.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "KING EDWARD IV:\n",
            "This cast, you tell me, yea, and you, my God for what said\n",
            "He?\n",
            "\n",
            "TRANIO:\n",
            "Signior Baptista! do you this alliance\n",
            "Will slay thy matter to him, and I'll\n",
            "From the Went-onime,\n",
            "Were not besore. Your knees go: yet I do Nor\n",
            "come down, good citizens: you know none, how licking,\n",
            "As little counsel, let it be pl \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.546757936477661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pywcktLfLYLL",
        "outputId": "0ba62213-4c7a-4bf5-ff62-47bb45497b3f"
      },
      "source": [
        "start = time.time()\r\n",
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1000):\r\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(result, '\\n\\n' + '_'*80)\r\n",
        "\r\n",
        "\r\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHear and ha! let her be kept on him:\\nCall home, his nor tongues, travelling time\\nThan have found scaved by--adised eyes,\\nTo hurry for trimune from the warlike posture.\\n\\nPETRUCHIO:\\nNoble lord!\\n\\nEDIOLANUS:\\nDo so; anon, if you lord desire yourself\\nHe of yourself to come abund again.\\n\\nKING EDWARD IV:\\nWhy, then thou shalt become this fround intent,\\nBy Bauthop's tap toicora, that are mark for them.\\n\\nKING EDWARD IV:\\nThe cleager of mine honour, his noble issue\\nA toich of micheries that makes me here.\\nThat should by conscience takes upon each other.\\nAnd let him spake this tempoxery to streak the back\\nand her whose remedy: scroop about, even so\\nAnd datch out with the bitter\\nUpon his son shall bear a common\\nOf my fortune to your affrage and bring\\nWith treech and my repited to her.\\n\\nEXETES:\\nAccursed, here I up stay, order thy life,\\nI would deny her a strelt through 's thus;\\nto the labelts know, which way, this deed dot I have heard\\nThe scaful seles of remission,\\nFrom thy morning, or what I mercy,\"\n",
            " b\"ROMEO:\\nWhat say'st thou? thus 'twas a sou dulladed?\\nFor joyful should quench it, how far an else,\\nThan, since you will deliver Hereford as he\\ndared caiming here, to bid thee chance to nopphesh.\\n\\nESCALUS:\\nThanks, not thy namely--\\n\\nASCELUS:\\nI'll do my common tribunes.\\n\\nClown:\\nHis voice, stand before me.'\\nAnd, for the petty hege so long have proofs'd,\\nFor you this title, then, to God's ear;\\nHere living, to levy men, into a shower; to\\nairest his cusbost, since we cannot do't.\\nI will attend ying food at any be land!\\nOr, if I can rear.\\n\\nFather:\\nHa! let me have no more designs.\\n\\nANGELO:\\nI hope so. For what subdues may strong ruff with us.\\n\\nCLAUDIO:\\nSwear e'er it bed,\\nAnd look into the buriden. For the rebels himself,\\nAnd bid them beggar in her: though you\\nTo do them both,\\nTo fear him but a fortance\\nwith a lineward recreateful my care.\\n\\nLADY CAPULET:\\nScope of wonders than dogs than any mund\\nAre at the adventure that most well-meant.\\n\\nLADY ANNE:\\nTo bear the ground, till I all comfort you: if\\nyou inte\"\n",
            " b\"ROMEO:\\n'Tis not well straight.\\n\\nLEONTES:\\nYou undended my father's death.\\n\\nDUKE VINCENTIO:\\nWho hath continued the envious slanders Ere of Widow,\\nFear on Without boots with lib,\\nStill--but in the life-boad pinapories, knife,\\nInly by rich a sevolam homeful bias!\\nYour grace mightst clothe it in the prince. I do believe Mercuid;\\nHail, vow, in griebs passage and bed blood,\\nTo breathe by Nature and to prove him to the good.\\nFor her prince proud and self-look'd friends,\\nBeing not my gage, to know your motion.\\nSlave, I look'd on any mean most worthy are in\\nhisterfries, your words shall see it in your brother.\\nAnd do not stay betraining her fatcher's\\nfault, and bear the block o' the sepulchreb;\\nAnd, if thou hast advised bestowed on my hegging.\\nDeath, for the crown and virtuous spotted from me.\\n\\nJOHN OF GAUNT:\\nWhy, how the fix'd of Menenius, were a fellow.\\n\\nKING HENRY VI:\\nIn deeper is King Richard: be assured\\nWe cannot do it.\\n\\nClown:\\nHow comes ay! I am sorry of merily starmer in.\\n\\nLARTIUS:\\nThe contract\"\n",
            " b\"ROMEO:\\nWhy, she's warm dido; for that she's the bishon\\nWested me to order peers.\\n\\nGONZALO:\\nBut the mayor it had so, for their dooms.\\nI'll swear it,--as I often have a device,\\nTo know of winter to his fame:\\nThe king I will attend your grace a pleasure,\\nAdvantaged as ever match'd only judged.\\n\\nPETRUCHIO:\\nAnd made a hope-for tongue but one of the\\nprisoner?\\n\\nISABELLA:\\nI am no beat to open in company your face,\\nThat from this complaint we here or home,\\nDo cry upon 't.\\n\\nCORINIUS:\\nI know it well.\\nI'll the better and by Madgaret;\\nAnd both of snow in war with peace.\\nFor suffering up a tabe: bring out restor'd,\\nAs in what curtain thich, that thou hast provided\\nDestiny was done to her fair life imprisence\\nThan die for Rome. Your heart is full of sorrow,\\nShall said such flat leasure from your curses.\\n\\nGLOUCESTER:\\nAnd to the Coriolanus, witchcraft,\\nWho is love women born gold for inquire thee.\\n\\nBAPTISTA:\\nNow, lords, take leave and speak against the head.\\nThe heavens have done, think with all but adorots:\"\n",
            " b\"ROMEO:\\nThey seemeth with him to present fore;\\nMy grainded and Montague should be of Mont-partical; that a heavy head\\nWhich the duke my brought could make thee oath,\\nMy absence makes us from us thus have stain'd\\nyour comfort! What sund you to cleequest thy father?\\n\\nBAPTISTA:\\nTrue.\\n\\nPOMPEY:\\nI beseech you, and first beauty take the less,\\nCan conscient harsh in your own such grief.\\n\\nDUKE VINCENTIO:\\nBroking a thousand times have armours to us.\\n\\nBAPTISTA:\\nAffection! widow! you blast-from me\\nRequire ma metely to the night from him that hable lasts?\\nUpon the white was hor, be true: our geternight\\nAnd duty, the poor gentleman, if\\nI make riborizen to thy lord.\\n\\nQUEEN MARGARET:\\nThou pay to yours, my friends with selfsame wars? or,\\nMy fair ready suprew off these fellows?\\n\\nFirst Servingman:\\nI am gone to France than showeress.\\n\\nABRICHORS:\\nThy foolish sake! know you the leavers further than thy life as well\\nIn shall ply now to rejoice but kill the from his.\\n\\nTRANIO:\\nI'll tear thee again. I am non, for\\nthe \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.2968199253082275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klWhTH-2Lkw0"
      },
      "source": [
        "## Export the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64lm4ZuaLl95",
        "outputId": "28eaeb33-663f-4755-c240-7239a975e70d"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\r\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f8191eac0f0>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJGrJ_BuLqIi",
        "outputId": "2bbf3dd3-47c3-4c00-86d8-dc57426fd69f"
      },
      "source": [
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(100):\r\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f81915438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f81915438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f81915438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f81915438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Go thank thee, Kate, I knew your hands and take it for.\n",
            "I am on earth: yet I will teach a husband;\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i1PZiJ9TlWV",
        "outputId": "66028b95-bbcd-49ea-9de5-bf78ac8f1634"
      },
      "source": [
        "states = None\r\n",
        "next_char = tf.constant(['We are accounted poor citizens'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1009):\r\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are accounted poor citizens.\n",
            "\n",
            "JOLINA:\n",
            "Beseech you,--\n",
            "\n",
            "First Citizen:\n",
            "By prove itself: it best thou wrot thy lord, like an innocent million\n",
            "Troops of love and lookness to come upon me. O, a month\n",
            "More witchronce to conqueror and bow.\n",
            "We dare ere you to? but I do not know\n",
            "I mean, to do this fool.\n",
            "\n",
            "GREMIO:\n",
            "And promise the English corse is mere upon this,\n",
            "For certain word: your sorrow's rest I live,\n",
            "I would be cured by stem my out-to poison.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Ay, Warwick, with a villain, what is yond frilar,\n",
            "Avoided meat to beat both the banishmy of spirit\n",
            "Unto the mind of Montague.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "This chartel to your does.\n",
            "\n",
            "CORIOLANUS:\n",
            "Have I none? where gust him please,\n",
            "By charged Supl, God grant the bendned--\n",
            "\n",
            "THOMPOLINGBROKE:\n",
            "If it I was that elected thee!\n",
            "Jest ambitious Edward, peace by flatter\n",
            "There to a molehill, and all things changed for;\n",
            "If it be sponet high fortune and the beauty\n",
            "That, not believe to reliver.\n",
            "\n",
            "PETRUCHIO:\n",
            "You say it is no more. A' might I see my care\n",
            "To have some love and hold deead men, so you\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
