{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDWXp8ZnXOGqpnFPCTJvQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Udacity/blob/main/Frameworks/Pytorch/RNNs/Char%20RNN/anna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2nRDGUnGfV",
        "outputId": "1caa7343-a7fc-45ea-d92c-e2aea13f2cfa"
      },
      "source": [
        "!ls\r\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\r\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\r\n",
        "!apt-get update -qq 2>&1 > /dev/null\r\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\r\n",
        "from google.colab import auth\r\n",
        "auth.authenticate_user()\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "creds = GoogleCredentials.get_application_default()\r\n",
        "import getpass\r\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\r\n",
        "vcode = getpass.getpass()\r\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\r\n",
        "\r\n",
        "\r\n",
        "!mkdir -p drive\r\n",
        "!google-drive-ocamlfuse drive\r\n",
        "\r\n",
        "import os\r\n",
        "os.chdir(\"drive/Udacity/RNN/\")\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 146364 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.24-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "'Anna Karenina.txt'   name2lang.txt   rnn_20_epoch.net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQ-XITGyM8w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty7kf2OWyMNH"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1RbBYHyyRbt"
      },
      "source": [
        "## Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nxrr8xByQ3B"
      },
      "source": [
        "# open text file and read in data as `text`\r\n",
        "with open('Anna Karenina.txt', 'r') as f:\r\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq6EZ0GvyfYc",
        "outputId": "88c45fc1-b64d-4f8b-e567-5ff94ec18f6f"
      },
      "source": [
        "print(text[:101])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everything\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zzQpkHYyr5g"
      },
      "source": [
        "## Tokenization : Encode the text and map each character to an integer and vice versa\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaADwcaDytrQ",
        "outputId": "d19cd7c6-0c1f-45c3-a736-c902cb049a4a"
      },
      "source": [
        "\r\n",
        "# we create two dictionaries:\r\n",
        "# 1. int2char, which maps integers to characters\r\n",
        "# 2. char2int, which maps characters to unique integers\r\n",
        "chars = tuple(set(text))\r\n",
        "print(chars)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('p', 'l', 'u', 'd', ' ', 'a', '1', '4', 't', 'O', 'b', 'm', '\"', '2', 'Y', '?', ',', 's', 'B', 'S', 'D', '5', 'y', 'j', 'i', 'v', 'r', 'x', 'R', 'H', 'k', 'P', 'L', '6', '8', \"'\", ';', 'M', '-', ')', 'G', 'U', '%', 'X', 'N', 'T', '/', '\\n', ':', '.', '!', '7', '_', 'c', 'K', 'V', 'q', 'A', '&', '@', 'C', 'g', 'W', '(', 'z', 'J', 'n', 'f', 'E', 'I', 'Z', '0', '`', 'F', '3', '9', 'Q', 'o', '$', 'h', 'w', '*', 'e')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vha3_BF4zIop",
        "outputId": "02245957-f739-4ebb-8d75-ac28df36e117"
      },
      "source": [
        "int2char = dict(enumerate(chars))\r\n",
        "print(int2char)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'p', 1: 'l', 2: 'u', 3: 'd', 4: ' ', 5: 'a', 6: '1', 7: '4', 8: 't', 9: 'O', 10: 'b', 11: 'm', 12: '\"', 13: '2', 14: 'Y', 15: '?', 16: ',', 17: 's', 18: 'B', 19: 'S', 20: 'D', 21: '5', 22: 'y', 23: 'j', 24: 'i', 25: 'v', 26: 'r', 27: 'x', 28: 'R', 29: 'H', 30: 'k', 31: 'P', 32: 'L', 33: '6', 34: '8', 35: \"'\", 36: ';', 37: 'M', 38: '-', 39: ')', 40: 'G', 41: 'U', 42: '%', 43: 'X', 44: 'N', 45: 'T', 46: '/', 47: '\\n', 48: ':', 49: '.', 50: '!', 51: '7', 52: '_', 53: 'c', 54: 'K', 55: 'V', 56: 'q', 57: 'A', 58: '&', 59: '@', 60: 'C', 61: 'g', 62: 'W', 63: '(', 64: 'z', 65: 'J', 66: 'n', 67: 'f', 68: 'E', 69: 'I', 70: 'Z', 71: '0', 72: '`', 73: 'F', 74: '3', 75: '9', 76: 'Q', 77: 'o', 78: '$', 79: 'h', 80: 'w', 81: '*', 82: 'e'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEDX7bszzNE9",
        "outputId": "c2754506-bc90-4f3e-c46e-511629b46fab"
      },
      "source": [
        "char2int = {ch: ii for ii, ch in int2char.items()}\r\n",
        "print(char2int)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'p': 0, 'l': 1, 'u': 2, 'd': 3, ' ': 4, 'a': 5, '1': 6, '4': 7, 't': 8, 'O': 9, 'b': 10, 'm': 11, '\"': 12, '2': 13, 'Y': 14, '?': 15, ',': 16, 's': 17, 'B': 18, 'S': 19, 'D': 20, '5': 21, 'y': 22, 'j': 23, 'i': 24, 'v': 25, 'r': 26, 'x': 27, 'R': 28, 'H': 29, 'k': 30, 'P': 31, 'L': 32, '6': 33, '8': 34, \"'\": 35, ';': 36, 'M': 37, '-': 38, ')': 39, 'G': 40, 'U': 41, '%': 42, 'X': 43, 'N': 44, 'T': 45, '/': 46, '\\n': 47, ':': 48, '.': 49, '!': 50, '7': 51, '_': 52, 'c': 53, 'K': 54, 'V': 55, 'q': 56, 'A': 57, '&': 58, '@': 59, 'C': 60, 'g': 61, 'W': 62, '(': 63, 'z': 64, 'J': 65, 'n': 66, 'f': 67, 'E': 68, 'I': 69, 'Z': 70, '0': 71, '`': 72, 'F': 73, '3': 74, '9': 75, 'Q': 76, 'o': 77, '$': 78, 'h': 79, 'w': 80, '*': 81, 'e': 82}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlqjXlN5zjuD",
        "outputId": "9813f5c0-a225-4b13-bfc1-39f1fae5ee23"
      },
      "source": [
        "# encode the text\r\n",
        "encoded = np.array([char2int[ch] for ch in text])\r\n",
        "print(encoded, encoded.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[60 79  5 ... 17 49 47] (1985223,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVExFkp50dks",
        "outputId": "302d2957-b764-432d-9f67-1db739d8d9fe"
      },
      "source": [
        "print(text[:101])\r\n",
        "print(encoded[:101])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everything\n",
            "[60 79  5  0  8 82 26  4  6 47 47 47 29  5  0  0 22  4 67  5 11 24  1 24\n",
            " 82 17  4  5 26 82  4  5  1  1  4  5  1 24 30 82 36  4 82 25 82 26 22  4\n",
            "  2 66 79  5  0  0 22  4 67  5 11 24  1 22  4 24 17  4  2 66 79  5  0  0\n",
            " 22  4 24 66  4 24  8 17  4 77 80 66 47 80  5 22 49 47 47 68 25 82 26 22\n",
            "  8 79 24 66 61]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-t0IjuR1Boc"
      },
      "source": [
        "## Pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1FiiI761Czr"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\r\n",
        "    \r\n",
        "    # Initialize the the encoded array\r\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\r\n",
        "    \r\n",
        "    # Fill the appropriate elements with ones\r\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\r\n",
        "    \r\n",
        "    # Finally reshape it to get back to the original array\r\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\r\n",
        "    \r\n",
        "    return one_hot"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA8PPbC41S-0",
        "outputId": "ed1658ab-11dc-44c2-8596-d5d6d7e7c30d"
      },
      "source": [
        "# check that the function works as expected\r\n",
        "test_seq = np.array([[3, 5, 1]])\r\n",
        "one_hot = one_hot_encode(test_seq, 8)\r\n",
        "\r\n",
        "print(one_hot)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6m-Drw3-zIp"
      },
      "source": [
        "## Making training mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8wS8j4S_35j",
        "outputId": "0701b47f-6d49-404a-9795-1af4e2cda763"
      },
      "source": [
        "print(\"total charachters = \", len(encoded))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total charachters =  1985223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhg-J07U_Gi7",
        "outputId": "bcd22562-c53b-4937-e857-b492c7b82331"
      },
      "source": [
        "batch_size = 8\r\n",
        "seq_length = 50\r\n",
        "batch_size_total = batch_size * seq_length\r\n",
        "\r\n",
        "# total number of batches we can make\r\n",
        "n_batches = len(encoded)//batch_size_total\r\n",
        "print(\"Total number of batches is \", n_batches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of batches is  4963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgiCsE5X_fCF",
        "outputId": "41193e4b-be70-4c0f-b3aa-7e2f980c7ad3"
      },
      "source": [
        "arr = encoded\r\n",
        "# Keep only enough characters to make full batches\r\n",
        "arr = arr[:n_batches * batch_size_total]\r\n",
        "print(arr.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1985200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjcBhH9olMCN",
        "outputId": "27a965c7-b245-468e-d33e-83b3a1121349"
      },
      "source": [
        "# Reshape into batch_size rows\r\n",
        "arr = arr.reshape((batch_size, -1))\r\n",
        "print(arr.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 248150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlK78srdA3Jf",
        "outputId": "53e0d820-567b-44f6-9a63-cc75b9375d3e"
      },
      "source": [
        "# iterate through the array, one sequence at a time\r\n",
        "for n in range(0, arr.shape[1], seq_length):\r\n",
        "  # The features\r\n",
        "  x = arr[:, n:n+seq_length]\r\n",
        "  #print(x.shape)\r\n",
        "  # The targets, shifted by one\r\n",
        "  \r\n",
        "  y = np.zeros_like(x)\r\n",
        "  #print(y.shape)\r\n",
        "  try:\r\n",
        "    y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\r\n",
        "  except IndexError:\r\n",
        "    y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\r\n",
        "  if n==0:\r\n",
        "    print(x.shape)\r\n",
        "    print(y.shape)   "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 50)\n",
            "(8, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nstD67h-0pH"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\r\n",
        "    '''Create a generator that returns batches of size\r\n",
        "       batch_size x seq_length from arr.\r\n",
        "       \r\n",
        "       Arguments\r\n",
        "       ---------\r\n",
        "       arr: Array you want to make batches from\r\n",
        "       batch_size: Batch size, the number of sequences per batch\r\n",
        "       seq_length: Number of encoded chars in a sequence\r\n",
        "    '''\r\n",
        "    \r\n",
        "    batch_size_total = batch_size * seq_length\r\n",
        "    # total number of batches we can make\r\n",
        "    n_batches = len(arr)//batch_size_total\r\n",
        "    \r\n",
        "    # Keep only enough characters to make full batches\r\n",
        "    arr = arr[:n_batches * batch_size_total]\r\n",
        "    # Reshape into batch_size rows\r\n",
        "    arr = arr.reshape((batch_size, -1))\r\n",
        "    \r\n",
        "    # iterate through the array, one sequence at a time\r\n",
        "    for n in range(0, arr.shape[1], seq_length):\r\n",
        "        # The features\r\n",
        "        x = arr[:, n:n+seq_length]\r\n",
        "        # The targets, shifted by one\r\n",
        "        y = np.zeros_like(x)\r\n",
        "        try:\r\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\r\n",
        "        except IndexError:\r\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\r\n",
        "        yield x, y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez1uXMYcG-x0"
      },
      "source": [
        "## Test Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_1zuy3HFPck",
        "outputId": "3eca3ca6-3ae7-4550-9e6e-6ca55dc87f3e"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\r\n",
        "x, y = next(batches)\r\n",
        "print(x.shape, y.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 50) (8, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3tjxpruFVUY",
        "outputId": "cf5a8bd9-f7f3-4de2-ddad-085b5bb135db"
      },
      "source": [
        "# printing out the first 10 items in a sequence\r\n",
        "print('x\\n', x[:, :10])\r\n",
        "print('\\ny\\n', y[:, :10])\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[60 79  5  0  8 82 26  4  6 47]\n",
            " [17 77 66  4  8 79  5  8  4  5]\n",
            " [82 66  3  4 77 26  4  5  4 67]\n",
            " [17  4  8 79 82  4 53 79 24 82]\n",
            " [ 4 17  5 80  4 79 82 26  4  8]\n",
            " [53  2 17 17 24 77 66  4  5 66]\n",
            " [ 4 57 66 66  5  4 79  5  3  4]\n",
            " [ 9 10  1 77 66 17 30 22 49  4]]\n",
            "\n",
            "y\n",
            " [[79  5  0  8 82 26  4  6 47 47]\n",
            " [77 66  4  8 79  5  8  4  5  8]\n",
            " [66  3  4 77 26  4  5  4 67 77]\n",
            " [ 4  8 79 82  4 53 79 24 82 67]\n",
            " [17  5 80  4 79 82 26  4  8 82]\n",
            " [ 2 17 17 24 77 66  4  5 66  3]\n",
            " [57 66 66  5  4 79  5  3  4 17]\n",
            " [10  1 77 66 17 30 22 49  4 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN2XaqqkIrnB",
        "outputId": "dd5fb9da-2646-4b84-d01e-8147684bee2b"
      },
      "source": [
        "# check if GPU is available\r\n",
        "train_on_gpu = torch.cuda.is_available()\r\n",
        "if(train_on_gpu):\r\n",
        "    print('Training on GPU!')\r\n",
        "else: \r\n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX8KkxqGHGZk"
      },
      "source": [
        "## Defining the network with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcHflFpLHHoz"
      },
      "source": [
        "class CharRNN(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,drop_prob=0.5, lr=0.001):\r\n",
        "        super().__init__()\r\n",
        "        self.drop_prob = drop_prob\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.n_hidden = n_hidden\r\n",
        "        self.lr = lr\r\n",
        "        \r\n",
        "        # creating character dictionaries\r\n",
        "        self.chars = tokens\r\n",
        "        self.int2char = dict(enumerate(self.chars))\r\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\r\n",
        "        \r\n",
        "        ## TODO: define the LSTM\r\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \r\n",
        "                            dropout=drop_prob, batch_first=True)\r\n",
        "        \r\n",
        "        ## TODO: define a dropout layer\r\n",
        "        self.dropout = nn.Dropout(drop_prob)\r\n",
        "        \r\n",
        "        ## TODO: define the final, fully-connected output layer\r\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\r\n",
        "      \r\n",
        "    \r\n",
        "    def forward(self, x, hidden):\r\n",
        "        ''' Forward pass through the network. \r\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\r\n",
        "                \r\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\r\n",
        "        r_output, hidden = self.lstm(x, hidden)\r\n",
        "        \r\n",
        "        ## TODO: pass through a dropout layer\r\n",
        "        out = self.dropout(r_output)\r\n",
        "        \r\n",
        "        # Stack up LSTM outputs using view\r\n",
        "        # you may need to use contiguous to reshape the output\r\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\r\n",
        "        \r\n",
        "        ## TODO: put x through the fully-connected layer\r\n",
        "        out = self.fc(out)\r\n",
        "        \r\n",
        "        # return the final output and the hidden state\r\n",
        "        return out, hidden\r\n",
        "    \r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        ''' Initializes hidden state '''\r\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\r\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        \r\n",
        "        if (train_on_gpu):\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\r\n",
        "        else:\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\r\n",
        "        \r\n",
        "        return hidden"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aWkMtgPHuIw",
        "outputId": "9832532d-a863-4b8c-eca3-a03f9e85629d"
      },
      "source": [
        "# define and print the net\r\n",
        "n_hidden=512\r\n",
        "n_layers=2\r\n",
        "\r\n",
        "net = CharRNN(chars, n_hidden, n_layers)\r\n",
        "print(net)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yfDYt7rIX2N",
        "outputId": "7697f192-31d9-4a82-8f10-65266ef4c9e5"
      },
      "source": [
        "print(net.init_hidden(8)[0].shape)\r\n",
        "print(net.init_hidden(8)[1].shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 512])\n",
            "torch.Size([2, 8, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVwfDJAo2Odk"
      },
      "source": [
        "## Time to train\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf9JUaD32NZ4"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\r\n",
        "    ''' Training a network \r\n",
        "    \r\n",
        "        Arguments\r\n",
        "        ---------\r\n",
        "        \r\n",
        "        net: CharRNN network\r\n",
        "        data: text data to train the network\r\n",
        "        epochs: Number of epochs to train\r\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\r\n",
        "        seq_length: Number of character steps per mini-batch\r\n",
        "        lr: learning rate\r\n",
        "        clip: gradient clipping\r\n",
        "        val_frac: Fraction of data to hold out for validation\r\n",
        "        print_every: Number of steps for printing training and validation loss\r\n",
        "    \r\n",
        "    '''\r\n",
        "    net.train()\r\n",
        "    \r\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    \r\n",
        "    # create training and validation data\r\n",
        "    val_idx = int(len(data)*(1-val_frac))\r\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\r\n",
        "    \r\n",
        "    if(train_on_gpu):\r\n",
        "        net.cuda()\r\n",
        "    \r\n",
        "    counter = 0\r\n",
        "    n_chars = len(net.chars)\r\n",
        "    for e in range(epochs):\r\n",
        "        # initialize hidden state\r\n",
        "        h = net.init_hidden(batch_size)\r\n",
        "        \r\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\r\n",
        "            counter += 1\r\n",
        "            \r\n",
        "            # One-hot encode our data and make them Torch tensors\r\n",
        "            x = one_hot_encode(x, n_chars)\r\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\r\n",
        "            \r\n",
        "            if(train_on_gpu):\r\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\r\n",
        "\r\n",
        "            # Creating new variables for the hidden state, otherwise\r\n",
        "            # we'd backprop through the entire training history\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "            # zero accumulated gradients\r\n",
        "            net.zero_grad()\r\n",
        "            \r\n",
        "            # get the output from the model\r\n",
        "            output, h = net(inputs, h)\r\n",
        "            \r\n",
        "            # calculate the loss and perform backprop\r\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\r\n",
        "            loss.backward()\r\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\r\n",
        "            opt.step()\r\n",
        "            \r\n",
        "            # loss stats\r\n",
        "            if counter % print_every == 0:\r\n",
        "                # Get validation loss\r\n",
        "                val_h = net.init_hidden(batch_size)\r\n",
        "                val_losses = []\r\n",
        "                net.eval()\r\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\r\n",
        "                    # One-hot encode our data and make them Torch tensors\r\n",
        "                    x = one_hot_encode(x, n_chars)\r\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\r\n",
        "                    \r\n",
        "                    # Creating new variables for the hidden state, otherwise\r\n",
        "                    # we'd backprop through the entire training history\r\n",
        "                    val_h = tuple([each.data for each in val_h])\r\n",
        "                    \r\n",
        "                    inputs, targets = x, y\r\n",
        "                    if(train_on_gpu):\r\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\r\n",
        "\r\n",
        "                    output, val_h = net(inputs, val_h)\r\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\r\n",
        "                \r\n",
        "                    val_losses.append(val_loss.item())\r\n",
        "                \r\n",
        "                net.train() # reset to train mode after iterationg through validation data\r\n",
        "                \r\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\"Step: {}...\".format(counter),\"Loss: {:.4f}...\".format(loss.item()),\"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3RAX1hj25lo"
      },
      "source": [
        "## Instantiating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab8i6X_o27ua",
        "outputId": "cd3f864f-8dd6-447f-c069-adb616fdc4f1"
      },
      "source": [
        "# define and print the net\r\n",
        "n_hidden=512\r\n",
        "n_layers=2\r\n",
        "\r\n",
        "net = CharRNN(chars, n_hidden, n_layers)\r\n",
        "print(net)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mKbPRqI3G7x",
        "outputId": "794d6748-1611-4001-c579-601388e8e720"
      },
      "source": [
        "batch_size = 128\r\n",
        "seq_length = 100\r\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\r\n",
        "\r\n",
        "# train the model\r\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2675... Val Loss: 3.2365\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1425... Val Loss: 3.1367\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1421... Val Loss: 3.1244\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1161... Val Loss: 3.1190\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1447... Val Loss: 3.1173\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1203... Val Loss: 3.1167\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1082... Val Loss: 3.1153\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1239... Val Loss: 3.1133\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1248... Val Loss: 3.1076\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1014... Val Loss: 3.0950\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0801... Val Loss: 3.0631\n",
            "Epoch: 1/20... Step: 120... Loss: 2.9890... Val Loss: 2.9908\n",
            "Epoch: 1/20... Step: 130... Loss: 2.9565... Val Loss: 2.9351\n",
            "Epoch: 2/20... Step: 140... Loss: 2.8895... Val Loss: 2.9751\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8173... Val Loss: 2.7853\n",
            "Epoch: 2/20... Step: 160... Loss: 2.7080... Val Loss: 2.6747\n",
            "Epoch: 2/20... Step: 170... Loss: 2.5882... Val Loss: 2.5649\n",
            "Epoch: 2/20... Step: 180... Loss: 2.5333... Val Loss: 2.4997\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4694... Val Loss: 2.4840\n",
            "Epoch: 2/20... Step: 200... Loss: 2.4645... Val Loss: 2.4485\n",
            "Epoch: 2/20... Step: 210... Loss: 2.4145... Val Loss: 2.4224\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3841... Val Loss: 2.3972\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3638... Val Loss: 2.3644\n",
            "Epoch: 2/20... Step: 240... Loss: 2.3460... Val Loss: 2.3380\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2822... Val Loss: 2.2888\n",
            "Epoch: 2/20... Step: 260... Loss: 2.2595... Val Loss: 2.2596\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2973... Val Loss: 2.2595\n",
            "Epoch: 3/20... Step: 280... Loss: 2.2707... Val Loss: 2.2296\n",
            "Epoch: 3/20... Step: 290... Loss: 2.2282... Val Loss: 2.1998\n",
            "Epoch: 3/20... Step: 300... Loss: 2.2079... Val Loss: 2.1778\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1887... Val Loss: 2.1582\n",
            "Epoch: 3/20... Step: 320... Loss: 2.1531... Val Loss: 2.1367\n",
            "Epoch: 3/20... Step: 330... Loss: 2.1240... Val Loss: 2.1200\n",
            "Epoch: 3/20... Step: 340... Loss: 2.1357... Val Loss: 2.1052\n",
            "Epoch: 3/20... Step: 350... Loss: 2.1270... Val Loss: 2.0825\n",
            "Epoch: 3/20... Step: 360... Loss: 2.0651... Val Loss: 2.0666\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0825... Val Loss: 2.0457\n",
            "Epoch: 3/20... Step: 380... Loss: 2.0631... Val Loss: 2.0290\n",
            "Epoch: 3/20... Step: 390... Loss: 2.0308... Val Loss: 2.0148\n",
            "Epoch: 3/20... Step: 400... Loss: 2.0036... Val Loss: 1.9965\n",
            "Epoch: 3/20... Step: 410... Loss: 2.0129... Val Loss: 1.9791\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9994... Val Loss: 1.9672\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9889... Val Loss: 1.9487\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9675... Val Loss: 1.9372\n",
            "Epoch: 4/20... Step: 450... Loss: 1.9035... Val Loss: 1.9211\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8959... Val Loss: 1.9098\n",
            "Epoch: 4/20... Step: 470... Loss: 1.9263... Val Loss: 1.8972\n",
            "Epoch: 4/20... Step: 480... Loss: 1.9030... Val Loss: 1.8819\n",
            "Epoch: 4/20... Step: 490... Loss: 1.9085... Val Loss: 1.8710\n",
            "Epoch: 4/20... Step: 500... Loss: 1.9023... Val Loss: 1.8577\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8691... Val Loss: 1.8462\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8782... Val Loss: 1.8326\n",
            "Epoch: 4/20... Step: 530... Loss: 1.8379... Val Loss: 1.8287\n",
            "Epoch: 4/20... Step: 540... Loss: 1.8100... Val Loss: 1.8143\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8529... Val Loss: 1.7983\n",
            "Epoch: 5/20... Step: 560... Loss: 1.8205... Val Loss: 1.7932\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7974... Val Loss: 1.7795\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7786... Val Loss: 1.7672\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7807... Val Loss: 1.7580\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7662... Val Loss: 1.7505\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7572... Val Loss: 1.7402\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7524... Val Loss: 1.7357\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7697... Val Loss: 1.7286\n",
            "Epoch: 5/20... Step: 640... Loss: 1.7335... Val Loss: 1.7177\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7234... Val Loss: 1.7125\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6958... Val Loss: 1.7009\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7245... Val Loss: 1.6984\n",
            "Epoch: 5/20... Step: 680... Loss: 1.7259... Val Loss: 1.6906\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6972... Val Loss: 1.6823\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6915... Val Loss: 1.6772\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6884... Val Loss: 1.6718\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6728... Val Loss: 1.6603\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6769... Val Loss: 1.6528\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6493... Val Loss: 1.6494\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6312... Val Loss: 1.6436\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6598... Val Loss: 1.6385\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6531... Val Loss: 1.6332\n",
            "Epoch: 6/20... Step: 780... Loss: 1.6380... Val Loss: 1.6261\n",
            "Epoch: 6/20... Step: 790... Loss: 1.6227... Val Loss: 1.6238\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6434... Val Loss: 1.6179\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6243... Val Loss: 1.6155\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5899... Val Loss: 1.6076\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6345... Val Loss: 1.6019\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5846... Val Loss: 1.5991\n",
            "Epoch: 7/20... Step: 850... Loss: 1.6009... Val Loss: 1.5936\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5934... Val Loss: 1.5842\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5924... Val Loss: 1.5808\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5946... Val Loss: 1.5778\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5972... Val Loss: 1.5737\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5736... Val Loss: 1.5705\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5426... Val Loss: 1.5648\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5683... Val Loss: 1.5591\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5484... Val Loss: 1.5569\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5569... Val Loss: 1.5527\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5685... Val Loss: 1.5488\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5611... Val Loss: 1.5433\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5681... Val Loss: 1.5416\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5393... Val Loss: 1.5441\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5401... Val Loss: 1.5446\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5336... Val Loss: 1.5305\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5660... Val Loss: 1.5288\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5373... Val Loss: 1.5267\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5199... Val Loss: 1.5243\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5368... Val Loss: 1.5243\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5067... Val Loss: 1.5166\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5128... Val Loss: 1.5145\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5145... Val Loss: 1.5085\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5122... Val Loss: 1.5034\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4993... Val Loss: 1.5011\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4890... Val Loss: 1.4962\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4961... Val Loss: 1.4972\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.5080... Val Loss: 1.4977\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.5012... Val Loss: 1.4963\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.5040... Val Loss: 1.4865\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5207... Val Loss: 1.4855\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4726... Val Loss: 1.4834\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4802... Val Loss: 1.4788\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4657... Val Loss: 1.4800\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.5044... Val Loss: 1.4766\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4483... Val Loss: 1.4736\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4672... Val Loss: 1.4685\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4634... Val Loss: 1.4680\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4452... Val Loss: 1.4645\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4499... Val Loss: 1.4607\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4613... Val Loss: 1.4598\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4674... Val Loss: 1.4559\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4515... Val Loss: 1.4561\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4733... Val Loss: 1.4512\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4536... Val Loss: 1.4504\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4488... Val Loss: 1.4515\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4613... Val Loss: 1.4458\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4221... Val Loss: 1.4492\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4305... Val Loss: 1.4439\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4198... Val Loss: 1.4399\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.4089... Val Loss: 1.4347\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4074... Val Loss: 1.4369\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.4116... Val Loss: 1.4345\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4413... Val Loss: 1.4329\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4517... Val Loss: 1.4297\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4517... Val Loss: 1.4281\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4550... Val Loss: 1.4261\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4497... Val Loss: 1.4247\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4112... Val Loss: 1.4249\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4387... Val Loss: 1.4195\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3695... Val Loss: 1.4180\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3980... Val Loss: 1.4142\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3945... Val Loss: 1.4138\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3992... Val Loss: 1.4065\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3930... Val Loss: 1.4075\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3795... Val Loss: 1.4106\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3709... Val Loss: 1.4065\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.4049... Val Loss: 1.3982\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4563... Val Loss: 1.4036\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.4053... Val Loss: 1.3982\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4123... Val Loss: 1.3955\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4175... Val Loss: 1.3922\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3679... Val Loss: 1.4009\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3412... Val Loss: 1.3927\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3429... Val Loss: 1.3898\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3759... Val Loss: 1.3883\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3606... Val Loss: 1.3921\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3564... Val Loss: 1.3820\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3816... Val Loss: 1.3807\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3549... Val Loss: 1.3826\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3283... Val Loss: 1.3755\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3850... Val Loss: 1.3724\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3639... Val Loss: 1.3796\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3667... Val Loss: 1.3693\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3481... Val Loss: 1.3753\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3528... Val Loss: 1.3678\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3309... Val Loss: 1.3669\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3326... Val Loss: 1.3651\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3685... Val Loss: 1.3615\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3422... Val Loss: 1.3655\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3164... Val Loss: 1.3703\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3377... Val Loss: 1.3559\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3636... Val Loss: 1.3697\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3370... Val Loss: 1.3623\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3213... Val Loss: 1.3632\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3433... Val Loss: 1.3543\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3553... Val Loss: 1.3578\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3380... Val Loss: 1.3544\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3555... Val Loss: 1.3488\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2947... Val Loss: 1.3507\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2790... Val Loss: 1.3494\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3440... Val Loss: 1.3464\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3435... Val Loss: 1.3474\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3243... Val Loss: 1.3432\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3528... Val Loss: 1.3496\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3294... Val Loss: 1.3423\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3211... Val Loss: 1.3470\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3306... Val Loss: 1.3518\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2866... Val Loss: 1.3412\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3366... Val Loss: 1.3383\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3069... Val Loss: 1.3409\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3050... Val Loss: 1.3400\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3011... Val Loss: 1.3343\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3034... Val Loss: 1.3342\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2956... Val Loss: 1.3381\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2849... Val Loss: 1.3419\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3087... Val Loss: 1.3341\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3353... Val Loss: 1.3397\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2929... Val Loss: 1.3329\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.2947... Val Loss: 1.3283\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2886... Val Loss: 1.3297\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2963... Val Loss: 1.3341\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3012... Val Loss: 1.3258\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2983... Val Loss: 1.3297\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.2973... Val Loss: 1.3312\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2860... Val Loss: 1.3215\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2794... Val Loss: 1.3252\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2951... Val Loss: 1.3223\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2689... Val Loss: 1.3197\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2748... Val Loss: 1.3169\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3087... Val Loss: 1.3168\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2796... Val Loss: 1.3173\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2740... Val Loss: 1.3213\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2793... Val Loss: 1.3156\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3036... Val Loss: 1.3161\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2768... Val Loss: 1.3155\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2351... Val Loss: 1.3150\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2890... Val Loss: 1.3164\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2531... Val Loss: 1.3160\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2780... Val Loss: 1.3113\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2484... Val Loss: 1.3104\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2625... Val Loss: 1.3164\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2776... Val Loss: 1.3102\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2727... Val Loss: 1.3083\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2794... Val Loss: 1.3060\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2464... Val Loss: 1.3049\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2622... Val Loss: 1.3106\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2586... Val Loss: 1.3044\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2553... Val Loss: 1.3069\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2705... Val Loss: 1.3092\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2763... Val Loss: 1.3040\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2787... Val Loss: 1.3106\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2502... Val Loss: 1.3111\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2491... Val Loss: 1.3012\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2504... Val Loss: 1.3017\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2868... Val Loss: 1.3112\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2780... Val Loss: 1.3030\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2543... Val Loss: 1.3006\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2629... Val Loss: 1.3036\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2499... Val Loss: 1.2987\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2391... Val Loss: 1.2982\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2538... Val Loss: 1.2946\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2490... Val Loss: 1.2948\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2390... Val Loss: 1.2952\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2379... Val Loss: 1.2969\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2443... Val Loss: 1.2968\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2524... Val Loss: 1.2925\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2616... Val Loss: 1.2922\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2649... Val Loss: 1.2928\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2805... Val Loss: 1.2913\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2385... Val Loss: 1.2931\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2554... Val Loss: 1.2861\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2359... Val Loss: 1.2910\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2727... Val Loss: 1.2921\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2292... Val Loss: 1.2898\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2314... Val Loss: 1.2857\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2314... Val Loss: 1.2919\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2161... Val Loss: 1.2889\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2259... Val Loss: 1.2929\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2522... Val Loss: 1.2858\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2415... Val Loss: 1.2873\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2558... Val Loss: 1.2846\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2507... Val Loss: 1.2866\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2455... Val Loss: 1.2852\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2258... Val Loss: 1.2879\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2484... Val Loss: 1.2830\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2032... Val Loss: 1.2862\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2204... Val Loss: 1.2820\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2062... Val Loss: 1.2808\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2069... Val Loss: 1.2829\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2104... Val Loss: 1.2855\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2058... Val Loss: 1.2856\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2473... Val Loss: 1.2844\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2619... Val Loss: 1.2833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lNkURhe6S2X"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXTm5ATl6VqS"
      },
      "source": [
        "# change the name, for saving multiple files\r\n",
        "model_name = 'rnn_20_epoch.net'\r\n",
        "\r\n",
        "checkpoint = {'n_hidden': net.n_hidden, 'n_layers': net.n_layers, 'state_dict': net.state_dict(),'tokens': net.chars}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kocp9_T6iAu"
      },
      "source": [
        "with open(model_name, 'wb') as f:\r\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVvHlkCz642D"
      },
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2n7VoGS67HH"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\r\n",
        "        ''' Given a character, predict the next character.\r\n",
        "            Returns the predicted character and the hidden state.\r\n",
        "        '''\r\n",
        "        \r\n",
        "        # tensor inputs\r\n",
        "        x = np.array([[net.char2int[char]]])\r\n",
        "        x = one_hot_encode(x, len(net.chars))\r\n",
        "        inputs = torch.from_numpy(x)\r\n",
        "        \r\n",
        "        if(train_on_gpu):\r\n",
        "            inputs = inputs.cuda()\r\n",
        "        \r\n",
        "        # detach hidden state from history\r\n",
        "        h = tuple([each.data for each in h])\r\n",
        "        # get the output of the model\r\n",
        "        out, h = net(inputs, h)\r\n",
        "\r\n",
        "        # get the character probabilities\r\n",
        "        p = F.softmax(out, dim=1).data\r\n",
        "        if(train_on_gpu):\r\n",
        "            p = p.cpu() # move to cpu\r\n",
        "        \r\n",
        "        # get top characters\r\n",
        "        if top_k is None:\r\n",
        "            top_ch = np.arange(len(net.chars))\r\n",
        "        else:\r\n",
        "            p, top_ch = p.topk(top_k)\r\n",
        "            top_ch = top_ch.numpy().squeeze()\r\n",
        "        \r\n",
        "        # select the likely next character with some element of randomness\r\n",
        "        p = p.numpy().squeeze()\r\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\r\n",
        "        \r\n",
        "        # return the encoded value of the predicted char and the hidden state\r\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xQLiNcS9FuB"
      },
      "source": [
        "## Priming and generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYghyCC97Aeu"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\r\n",
        "        \r\n",
        "    if(train_on_gpu):\r\n",
        "        net.cuda()\r\n",
        "    else:\r\n",
        "        net.cpu()\r\n",
        "    \r\n",
        "    net.eval() # eval mode\r\n",
        "    \r\n",
        "    # First off, run through the prime characters\r\n",
        "    chars = [ch for ch in prime]\r\n",
        "    h = net.init_hidden(1)\r\n",
        "    for ch in prime:\r\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\r\n",
        "\r\n",
        "    chars.append(char)\r\n",
        "    \r\n",
        "    # Now pass in the previous character and get a new one\r\n",
        "    for ii in range(size):\r\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\r\n",
        "        chars.append(char)\r\n",
        "\r\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffMPP_Dg9Plj",
        "outputId": "cca3104a-d162-4ebc-effd-14e2757e8b20"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna. The\n",
            "different deat of the creature, who came to her three sides, said\n",
            "simple to be forgattening the country, and as she was set on\n",
            "the praces of a secret of in a compasisance of thirt that he had been in such\n",
            "a son, as the side was senst that they were a great feelings,\n",
            "and that he was asking all the carry of the mississ, and were this\n",
            "starmed of the crubler, triuper, while the conversation was\n",
            "even seeing her and settle had she standed out the same any courters,\n",
            "sount from the memory, and with a smile was so as had there wished\n",
            "to\n",
            "see him with his secret attires and their friends and something at\n",
            "a last thrie of society and stoping the close time. At happiness\n",
            "were almost feeling to all themer and starter, the carriage wished and\n",
            "she was not because his wife they were starting, but he cannot had\n",
            "been tabered for the pass of his feet and saying, and all of his\n",
            "head one hand on the point.\n",
            "\n",
            "Seriey Ivanovitch sat down.\n",
            "\n",
            "\"You're not so much,\" he said, trying to see the drows of the mother\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPYqx0-t9gGk"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1--Pa079hZZ"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\r\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\r\n",
        "    checkpoint = torch.load(f)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTogXBKo9qan",
        "outputId": "717421f2-5929-4abc-aea9-2215f4ca63bf"
      },
      "source": [
        "net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\r\n",
        "net.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Nib98I9t0_"
      },
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmsAV8Kp9tKt",
        "outputId": "900c7be8-0cda-43ee-fb7a-8b58491d44f1"
      },
      "source": [
        "# Sample using a loaded model\r\n",
        "print(sample(net, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said\n",
            "that their subjection was she had been trying to drop the chince to\n",
            "and that had, as she could be standing to him, and all, and the peasants\n",
            "had as all sevarally said that all he had nothing, and what is it through\n",
            "his weeling. The same stepper satisfacted and all at the mother was\n",
            "attention, to be talking of the child, and stared in the significance\n",
            "of the days, he had to grieve the mother, he could not have come to\n",
            "her, while he had been so much he saw that he would nut have saw that\n",
            "she had been made to his head, and, as it had said sight togother that\n",
            "the choode was the carriage, there.\n",
            "\n",
            "\"I don't know you that I don't let it in the middle of the stathing of it, and\n",
            "who's a sort of sort of the same to think about to that,.\"\n",
            "\n",
            "\"It's seeing her what there's a singer, but I suppress him abroad,\n",
            "so all that you must give it,\" she said, \"then you don't know, and what's that you are\n",
            "not intensily to see.\"\n",
            "\n",
            "\"You do, and then the sound of all, in his words whone one of the princess,\n",
            "I don't know her, and shall not better not too seem and\n",
            "so long while the chief thoughts is this way thas she\n",
            "could not go and been seeming to me full about, and I shall give her a\n",
            "little at the family tones of her fresh. The man of my position on\n",
            "the country, and at it it was a man, and where you can't see you alone.\"\n",
            "\n",
            "The lovel, was still to be at her.\n",
            "\n",
            "The mare was the property was interest and distinctly to the marsh and\n",
            "which had thought and thinking about his finger on the same across that huse\n",
            "that he could not spoke a clower of his companiance and he had\n",
            "still she wanted to be sold. He could not be seen at once the\n",
            "memories of the same friend of secine and his hands, brightly at him\n",
            "to say, and he had been the same was a long of an ears, which had seen\n",
            "such teller. Between his bride of her stall of her sister of his\n",
            "face as the side that had been the signifient farmer as he came the\n",
            "steps, he had tried to say to the strong of the carriage in his\n",
            "heart.\n",
            "\n",
            "\"Yes, you are said, it's not fro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8krFDSLy-e5F",
        "outputId": "4fd0e108-6ba3-475a-be38-a3a9c411cb96"
      },
      "source": [
        "# Sample using a loaded model\r\n",
        "print(sample(net, 2000, top_k=5, prime=\"Everything was in confusion\"))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything was in confusion in\n",
            "with her that seemed to be at haste to see. And this same the\n",
            "servom he sat this all thas she was saying and were sitting at the sight.\n",
            "\n",
            "\"And how could there all do it?\"\n",
            "\n",
            "\"We are not so sincere, and I would shoul the meeting in the\n",
            "cliss of the face.\"\n",
            "\n",
            "\"Why don it, and the pirter that I do not allow that they, I can't, I'll\n",
            "be done, and thank of the same ten, all to see it than some\n",
            "one of the carm, and as I want to say something, and I have\n",
            "so such as too tell me and see, and then.\"\n",
            "\n",
            "The carriage would stand to have teacher their people.\n",
            "The conversation was stending that the princess was a prescust. She went\n",
            "to herself, his stale that in all of a misurder trater the princess. Has\n",
            "spiting at home, and he was, that still materable minute of seemen\n",
            "servant he had been doibing the same tailing or on her.\n",
            "\n",
            "\"You can't get out to might to tell you that he has been served. That master,\n",
            "there were now so as to be so.\"\n",
            "\n",
            "\"Oh, then, it's a lady of making out of my change that she can be\n",
            "so talking over to tear, and there have so much time.\n",
            "Why so you said,\" he said.\n",
            "\n",
            "\"If you don't care and do wasts. I wanted to blome him, I don't know\n",
            "that though he wrong the carriage and to blame, as this wands in\n",
            "the center, but I saw the more faminiar son that I see how to tried you\n",
            "will be all the charm of some of a masser, and thanking there was the\n",
            "consciousness of matter of hill between the face, and when I don't\n",
            "wanc the same the carriage, it came up for the same and man in the\n",
            "corress and herself,\" said Levin, till something, he was that her\n",
            "fact the princess had that he had told him to see her that he did not\n",
            "cut him would seem any same side that the setter saw was the\n",
            "collar attention to him.\n",
            "\n",
            "\"Would you see that it was a conversation and more as a latter friends.\"\n",
            "\n",
            "\"Oh, no, I should be such such a far and done that I can't give me,\n",
            "I'll go to the plough. He'll be no deading of him; I\n",
            "was a seriage as a courte of hands and dear...\" he went into a\n",
            "listening at the stard wat the s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}